{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7247c86c-658e-484b-819b-7c93004a1018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import Pipe\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a57a9f-5063-48cc-ad75-c23f98e35f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Globals strings ###\n",
    "ONTONOTES_LABELS = [\n",
    "    'CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL',\n",
    "    'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "\n",
    "ontonotes_json = \"data/ontonotes5_en_16percent.json\"\n",
    "\n",
    "example_text = \"On March 8, 2021, a group of hackers including Kottmann and calling themselves \" \\\n",
    "       \"'APT - 69420 Arson Cats' gained 'super admin' rights in the network of Verkada, a \" \\\n",
    "       \"cloud-based security camera company, using credentials they found on the public \" \\\n",
    "       \"internet. They had access to the network for 36 hours. The group collected about 5 \" \\\n",
    "       \"gigabytes of data, including live security camera footage and recordings from more\" \\\n",
    "       \" than 150,000 cameras in places like a Tesla factory, a jail in Alabama, a Halifax \" \\\n",
    "       \"Health hospital, and residential homes. The group also accessed a list of Verkada \" \\\n",
    "       \"customers and the company's private financial information, and gained superuser \" \\\n",
    "       \"access to the corporate networks of Cloudflare and Okta through their Verkada cameras.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dab122-88b8-4b10-8c9e-cf022b3dc15b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def setup_argparse():\n",
    "#     p = argparse.ArgumentParser()\n",
    "#     p.add_argument('--part', choices=['1','2','3','4'], required=True)\n",
    "#     p.add_argument('--ents', choices=ONTONOTES_LABELS, nargs='+')\n",
    "#     p.add_argument('--viz_output', default='entity_viz_example.html',\n",
    "#                    help='Name of output file for the visualisation')\n",
    "#     p.add_argument('--corpus', help='name of corpus file to load in')\n",
    "#     p.add_argument('--tokenization', choices=['standard', 'subword'], default='standard',\n",
    "#                    help='for part 2, whether to print out the tokenized document in standard'\n",
    "#                         'tokenization (whitespace), or showing subwords (BPE)')\n",
    "#     p.add_argument('--classifier_path', help='name for path to save classifier')\n",
    "#     p.add_argument('--baseline', action='store_true', help='use a simple baseline classifier')\n",
    "#     return p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcc38e9-56b8-4407-9174-e2c78cca8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_argparse():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--part', choices=['1','2','3','4'], required=True)\n",
    "    p.add_argument('--ents', choices=ONTONOTES_LABELS, nargs='+')\n",
    "    p.add_argument('--viz_output', default='entity_viz_example.html',\n",
    "                   help='Name of output file for the visualisation')\n",
    "    p.add_argument('--corpus', help='name of corpus file to load in')\n",
    "    p.add_argument('--tokenization', choices=['standard', 'subword'], default='standard',\n",
    "                   help='for part 2, whether to print out the tokenized document in standard'\n",
    "                        'tokenization (whitespace), or showing subwords (BPE)')\n",
    "    p.add_argument('--classifier_path', help='name for path to save classifier')\n",
    "    p.add_argument('--baseline', action='store_true', help='use a simple baseline classifier')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a2c50274-b0c9-497f-8aea-ed4c9c4b2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "def part_1(args, nlp):\n",
    "    doc = nlp(example_text)\n",
    "    ent_list, output_file = args.ents, args.viz_output\n",
    "    options = {\"ents\": ent_list} if ent_list else {\"ents\": ONTONOTES_LABELS}\n",
    "\n",
    "    html = displacy.render(doc, style=\"ent\", options=options)\n",
    "\n",
    "    # output_path = Path(output_file)\n",
    "    # output_path.open(\"w\").write(html)\n",
    "\n",
    "\n",
    "def part_2(args, nlp):\n",
    "    # These are special characters used by the tokenizer, ignore them\n",
    "    special_chars = re.compile(\"Ġ|<pad>|<s>|</s>|â|Ģ|ī\")\n",
    "    doc = nlp(example_text)\n",
    "\n",
    "    print(\"List of Entities:\")\n",
    "    print(doc.ents)\n",
    "\n",
    "    if args.tokenization == 'standard':\n",
    "        print(\"\\nStandard Tokenization:\")\n",
    "        print(\" \".join([tok.text for tok in doc]))\n",
    "\n",
    "    elif args.tokenization == 'subword':\n",
    "        print(\"\\nSubword Tokenization:\")\n",
    "        subword_string = \" \".join([tok for tok in itertools.chain(*doc._.trf_data.wordpieces.strings)])\n",
    "        cleaned_subword_string = special_chars.sub(\"\", subword_string).strip()\n",
    "\n",
    "        print(cleaned_subword_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c0b8f5-31d3-48c7-9694-5ae6e5a9c075",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This is for Part 3 ###\n",
    "class ContextualVectors(Pipe):\n",
    "    def __init__(self, nlp):\n",
    "        self._nlp = nlp\n",
    "        self.combination_function = \"average\"\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        if type(doc) == str:\n",
    "            doc = self._nlp(doc)\n",
    "        self.lengths = doc._.trf_data.align.lengths\n",
    "        self.tensors = doc._.trf_data.tensors\n",
    "        self.input_texts = doc._.trf_data.tokens['input_texts'][0]\n",
    "        doc.user_token_hooks[\"vector\"] = self.vector\n",
    "        return doc\n",
    "\n",
    "    ### HERE is where vectors are set\n",
    "    def vector(self, token):\n",
    "\n",
    "        token_start_idx = 1 + sum([self.lengths[ii] for ii in range(token.i)])\n",
    "        token_end_idx = token_start_idx + self.lengths[token.i]\n",
    "        trf_vector = self.tensors[0][0][token_start_idx:token_end_idx]\n",
    "        \n",
    "        if len(trf_vector) == 0: # this happens due to token alignment issues\n",
    "            # print('len(trf_vector) = 0!')\n",
    "            # print(token_start_idx, token_end_idx)\n",
    "            # print(len(self.tensors[0][0]))\n",
    "            # print('token.i:', token.i, token.text)\n",
    "            # print('token_idx:', token_start_idx, token_end_idx)\n",
    "            # print('input_texts', self.input_texts[token_start_idx:token_end_idx])\n",
    "            return []\n",
    "            \n",
    "        return self.combine_vectors(trf_vector)\n",
    "\n",
    "    def combine_vectors(self, trf_vector):\n",
    "        return np.average(trf_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daaf8ef2-ccb9-4808-81ca-3a536a36d438",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@Language.factory(\"trf_vector_hook\", assigns=[\"doc.user_token_hooks\"])\n",
    "def create_contextual_hook(nlp, name):\n",
    "    return ContextualVectors(nlp)\n",
    "\n",
    "\n",
    "def part_3(args, nlp):\n",
    "\n",
    "    nlp.add_pipe(\"trf_vector_hook\", last=True)\n",
    "    max_tok = 145  # max tokens per chunk based on the spacy striding behaviour. I can change this if I want\n",
    "    def chunks(tokens, n):\n",
    "        for i in range(0, len(tokens), n):\n",
    "            yield tokens[i:i+n]\n",
    "\n",
    "\n",
    "    with open(ontonotes_json) as fin:\n",
    "        f = json.load(fin)\n",
    "    # process all the data\n",
    "    corpus = dict.fromkeys(f.keys())\n",
    "    for key in f.keys():\n",
    "        #print(\"loading {}\".format(key))\n",
    "        embeddings, labels = [], []\n",
    "        corpus_split = f[key]\n",
    "        for entry in tqdm(corpus_split, desc=f\"Processing {key}\"):\n",
    "            if not entry.get(\"entities\"):\n",
    "                continue\n",
    "            this_string = entry[\"text\"]\n",
    "            # BERT max is 512 wordpiece tokens at once, and there is one sample that exceeeds it\n",
    "            if len(this_string.split()) > max_tok:\n",
    "                text_chunks = chunks(this_string, max_tok)\n",
    "            else:\n",
    "                text_chunks = [this_string]\n",
    "            for c in text_chunks:\n",
    "                this_doc = nlp(\"\".join(c))\n",
    "                # for silver labels:\n",
    "                for ent in this_doc.ents:\n",
    "                    \n",
    "                    try:\n",
    "                        if not ent.vector.any(): \n",
    "                            continue\n",
    "                    except:\n",
    "                        # print(f\"Error on entity '{ent}' in document: {this_doc}\")\n",
    "                        # print('ent_idx:', ent.start, ent.end)\n",
    "                        continue\n",
    "                    # validation check for nans\n",
    "                    if np.isnan(ent.vector).any() or np.isinf(ent.vector.any()):\n",
    "                        print(f\"Skipping entry, found nan or inf in vector for entity '{ent}' \"\n",
    "                              f\"in document: {this_doc}\")\n",
    "                        continue\n",
    "                    embeddings.append(ent.vector)\n",
    "                    labels.append(ent.label_)\n",
    "        # save processed split of corpus, with matrix of number_samples x features, list of labels\n",
    "        corpus[key] = [np.vstack(embeddings), labels]\n",
    "\n",
    "    # print number of entities found in each section for information\n",
    "    for key in corpus.keys():\n",
    "        print(\"{}: {} entities\".format(key, len(corpus[key][0])))\n",
    "\n",
    "    save_file = \"data/corpus_average.pkl\"\n",
    "    with open(save_file, \"wb\") as fout:\n",
    "        pickle.dump(corpus, fout)\n",
    "\n",
    "    print(f\"Saved full processed corpus to {save_file}\")\n",
    "\n",
    "### Errors\n",
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
    "\n",
    "\n",
    "def print_classifier_stats(predictions: List[str], labels: List[str], classes: List[str]):\n",
    "    # TODO check if this works with NER confusion matrix and if it does make a higher and use twice\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    # matrix_labels = (ONTONOTES_LABELS\n",
    "    #     [label.name for label in Label] + [] if not conf_thresh else [label.name for label in\n",
    "    #                                                                   Label] + [\"below thresh\"]\n",
    "    # )\n",
    "    print(\"Classifier Accuracy: {}\".format(accuracy))\n",
    "    print(\"-\" * 89)\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(labels, predictions, target_names=classes, zero_division=0))\n",
    "    # TODO currently get a broadcast error, fix ValueError: shape mismatch: objects cannot be broadcast to a single shape\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(metrics.confusion_matrix(test_labels_, predictions_, labels=[label_encoder.classes_]))\n",
    "\n",
    "\n",
    "def part_4(args, nlp):\n",
    "    # this involves reading in ontonotes data, getting embeddings for the entities,\n",
    "    # then training a classifier with the paired embeddings and labels.\n",
    "    classifier = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        #class_weight=\"balanced\",\n",
    "        max_iter=500\n",
    "    )\n",
    "\n",
    "    # This loads a dict of TESTING, TRAINING, VALIDATION keys and values as a nested list of\n",
    "    # 0 as embeddings and 1 as labels (co-indexed, equal length)\n",
    "    with open(args.corpus, \"rb\") as fin:\n",
    "        corpus = pickle.load(fin)\n",
    "\n",
    "    # process data\n",
    "    label_encoder = preprocessing.LabelEncoder()  # labels need to be ints not strings\n",
    "    all_labels = list(itertools.chain(*[corpus[split][1] for split in corpus.keys()]))\n",
    "    label_encoder.fit(all_labels)\n",
    "\n",
    "    train_data, train_labels_ = corpus[\"TRAINING\"]  # the _ is the spacy convention for the string representation (rather than int/float)\n",
    "    test_data, test_labels_ = corpus[\"TESTING\"]\n",
    "\n",
    "    train_labels = label_encoder.transform(train_labels_)  # transform strings to ints\n",
    "\n",
    "    if args.baseline:\n",
    "        for strat in [\"most_frequent\", \"uniform\", \"stratified\"]:\n",
    "            dummy_classifier = DummyClassifier(strategy=strat)\n",
    "            dummy_classifier.fit(train_data, train_labels)\n",
    "            dummy_predictions = dummy_classifier.predict(test_data)\n",
    "            dummy_predictions_ = label_encoder.inverse_transform(dummy_predictions)\n",
    "\n",
    "            print(f\"Stats for Baseline Classifier: {strat} on Test Set\")\n",
    "            print_classifier_stats(dummy_predictions_, test_labels_, label_encoder.classes_)\n",
    "\n",
    "    else:\n",
    "        print(\"Training classifier with params:\")\n",
    "        print(classifier.get_params())\n",
    "        \n",
    "        if 'cupy' in str(type(train_data)):\n",
    "            train_data = train_data.get()\n",
    "            test_data = test_data.get()\n",
    "\n",
    "        classifier.fit(train_data, train_labels)\n",
    "\n",
    "        print(\"Saving classifier to {}\".format(args.classifier_path))\n",
    "        with open(args.classifier_path, \"wb\") as fout:\n",
    "            pickle.dump(classifier, fout)\n",
    "\n",
    "        predictions = classifier.predict(test_data)\n",
    "        predictions_ = label_encoder.inverse_transform(predictions)  # inverse transform to strings for printing\n",
    "\n",
    "        print(\"Stats for Logistic Regression Classifier on Test Set\")\n",
    "        print_classifier_stats(predictions_, test_labels_, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45e8e060-1196-43ac-a9d7-4d978f623ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, nlp):\n",
    "    dict2func = {\n",
    "        \"1\": part_1,\n",
    "        \"2\": part_2,\n",
    "        \"3\": part_3,\n",
    "        \"4\": part_4,\n",
    "    }\n",
    "\n",
    "    dict2func[args.part](args, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66ba49c-14ac-4a3e-a701-f7b44765517a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     args = setup_argparse()\n",
    "\n",
    "#     gpu = spacy.prefer_gpu()\n",
    "#     print('GPU:', gpu)\n",
    "\n",
    "#     # validation checks\n",
    "#     # that model is downloaded\n",
    "#     spacy_model_name = 'en_core_web_trf'\n",
    "#     if not spacy.util.is_package(spacy_model_name):\n",
    "#         spacy.cli.download(spacy_model_name)\n",
    "#     # that relevant directories exist\n",
    "#     for d in [\"models\", \"data\"]:\n",
    "#         if not os.path.exists(d):\n",
    "#             os.makedirs(d)\n",
    "\n",
    "#     # load spacy model\n",
    "#     nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "#     main(args, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60cb1a93-525d-44cc-8ff5-df32fa4b2faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lab8(arg):\n",
    "    p = init_argparse()\n",
    "    args = p.parse_args(arg)\n",
    "\n",
    "    gpu = spacy.prefer_gpu()\n",
    "    print('GPU:', gpu)\n",
    "\n",
    "    # validation checks\n",
    "    # that model is downloaded\n",
    "    spacy_model_name = 'en_core_web_trf'\n",
    "    if not spacy.util.is_package(spacy_model_name):\n",
    "        spacy.cli.download(spacy_model_name)\n",
    "    # that relevant directories exist\n",
    "    for d in [\"models\", \"data\"]:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    # load spacy model\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "    main(args, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8fb98-91cf-4461-8c3f-b91a4d9fc225",
   "metadata": {},
   "source": [
    "## Run lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "40201e64-280e-4177-a2c1-e6379edc3a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_core_web_trf' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">On \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    March 8, 2021\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", a group of hackers including \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kottmann\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and calling themselves 'APT - 69420 Arson Cats' gained 'super admin' rights in the network of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Verkada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a cloud-based security camera company, using credentials they found on the public internet. They had access to the network for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    36 hours\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       ". The group collected \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    about 5 gigabytes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " of data, including live security camera footage and recordings from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than 150,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " cameras in places like a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " factory, a jail in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alabama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Halifax Health\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " hospital, and residential homes. The group also accessed a list of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Verkada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " customers and the company's private financial information, and gained superuser access to the corporate networks of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cloudflare\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Okta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " through their \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Verkada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " cameras.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab8(['--part', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e60d0e-8a0c-4052-bff9-b06a937cc1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Entities:\n",
      "(March 8, 2021, Kottmann, Verkada, 36 hours, about 5 gigabytes, more than 150,000, Tesla, Alabama, Halifax Health, Verkada, Cloudflare, Okta, Verkada)\n",
      "\n",
      "Standard Tokenization:\n",
      "On March 8 , 2021 , a group of hackers including Kottmann and calling themselves ' APT   -   69420 Arson Cats ' gained ' super admin ' rights in the network of Verkada , a cloud - based security camera company , using credentials they found on the public internet . They had access to the network for 36 hours . The group collected about 5 gigabytes of data , including live security camera footage and recordings from more than 150,000 cameras in places like a Tesla factory , a jail in Alabama , a Halifax Health hospital , and residential homes . The group also accessed a list of Verkada customers and the company 's private financial information , and gained superuser access to the corporate networks of Cloudflare and Okta through their Verkada cameras .\n"
     ]
    }
   ],
   "source": [
    "lab8(['--part', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c092004-68f4-4e19-b04a-2e4f804893b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TESTING: 100%|██████████| 1734/1734 [00:24<00:00, 70.71it/s]\n",
      "Processing TRAINING: 100%|██████████| 17125/17125 [04:17<00:00, 66.62it/s] \n",
      "Processing VALIDATION: 100%|██████████| 2257/2257 [00:36<00:00, 62.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING: 1864 entities\n",
      "TRAINING: 19410 entities\n",
      "VALIDATION: 3070 entities\n",
      "Saved full processed corpus to data/corpus_average.pkl\n"
     ]
    }
   ],
   "source": [
    "# lab8(['--part', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd64c4df-7400-40f9-8dab-f7e7d1eec9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TESTING: 100%|██████████| 1734/1734 [00:14<00:00, 123.40it/s]\n",
      "Processing TRAINING: 100%|██████████| 17125/17125 [02:16<00:00, 125.44it/s]\n",
      "Processing VALIDATION: 100%|██████████| 2257/2257 [00:20<00:00, 109.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING: 1864 entities\n",
      "TRAINING: 19410 entities\n",
      "VALIDATION: 3070 entities\n",
      "Saved full processed corpus to data/corpus_average.pkl\n"
     ]
    }
   ],
   "source": [
    "lab8(['--part', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a88dee83-5c61-4e70-86b1-3750a48b38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n",
      "Training classifier with params:\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "Saving classifier to models/model_average.pkl\n",
      "Stats for Logistic Regression Classifier on Test Set\n",
      "Classifier Accuracy: 0.9393776824034334\n",
      "-----------------------------------------------------------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.93      0.95      0.94       151\n",
      "        DATE       0.95      0.95      0.95       301\n",
      "       EVENT       0.91      0.91      0.91        11\n",
      "         FAC       0.77      0.89      0.83        19\n",
      "         GPE       0.95      0.93      0.94       393\n",
      "    LANGUAGE       0.50      0.67      0.57         3\n",
      "         LAW       0.86      0.67      0.75         9\n",
      "         LOC       0.90      0.93      0.92        30\n",
      "       MONEY       1.00      1.00      1.00        48\n",
      "        NORP       0.86      0.89      0.88       131\n",
      "     ORDINAL       0.97      0.95      0.96        38\n",
      "         ORG       0.93      0.95      0.94       313\n",
      "     PERCENT       1.00      0.93      0.96        40\n",
      "      PERSON       0.97      0.98      0.97       297\n",
      "     PRODUCT       0.87      0.72      0.79        18\n",
      "    QUANTITY       1.00      0.89      0.94        19\n",
      "        TIME       0.97      0.97      0.97        31\n",
      " WORK_OF_ART       0.75      0.75      0.75        12\n",
      "\n",
      "    accuracy                           0.94      1864\n",
      "   macro avg       0.89      0.88      0.89      1864\n",
      "weighted avg       0.94      0.94      0.94      1864\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lab8(['--part', '4', '--classifier_path', 'models/model_average.pkl', '--corpus', 'data/corpus_average.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc80477-cd29-4b84-8da3-f396d452d28e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59478dde-cb69-4273-86af-413ea2872c22",
   "metadata": {},
   "source": [
    "### Part 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d551fc3b-236b-4c6b-98be-2b52c0e6006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6e96d5e-9862-48e1-9201-46da4d316acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6c0a0c9-5459-4b52-8b97-5ee216a6f35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e15b6e6-2719-4e6e-9906-ddbce4f5d08a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(March 8, 2021,\n",
       " Kottmann,\n",
       " Verkada,\n",
       " 36 hours,\n",
       " about 5 gigabytes,\n",
       " more than 150,000,\n",
       " Tesla,\n",
       " Alabama,\n",
       " Halifax Health,\n",
       " Verkada,\n",
       " Cloudflare,\n",
       " Okta,\n",
       " Verkada)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db029349-b99b-4a5c-8e1c-2553a11c8d23",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a358600a-664b-4ab7-ae26-cf38fb5d37b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x263a9068e20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x263a90860a0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x263cf74d970>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x263a6753cf0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x263aeec2c80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x263aeebc240>)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bfe9b618-5c14-4836-9f0b-411a16ee2e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextualVectors at 0x263b02a00c0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"trf_vector_hook\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a39ec756-2420-4ef8-bdbb-5937a6671dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x263a9068e20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x263a90860a0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x263cf74d970>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x263a6753cf0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x263aeec2c80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x263aeebc240>),\n",
       " ('trf_vector_hook', <__main__.ContextualVectors at 0x263b02a00c0>)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "73ca3ca4-d1ff-4524-ab44-6c777ca6ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ontonotes_json) as fin:\n",
    "    f = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f5738736-b9b2-42a4-9e9b-875809f44e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict.fromkeys(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eb6133ac-898f-446b-8328-6ad071b1dbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TESTING', 'TRAINING', 'VALIDATION'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2b63fe10-5d36-4609-9256-d195d0adda68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entry = f['TESTING'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e0ec0cec-34c0-42b8-8f8f-f51ce52a00ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'GPE': [[17, 27]], 'ORG': [[5, 13]], 'WORK_OF_ART': [[36, 66]]},\n",
       " 'language': 'english',\n",
       " 'morphology': {'$': [[66, 67]],\n",
       "  'DT': [[28, 32], [41, 44]],\n",
       "  'EX': [[9, 13]],\n",
       "  'IJ': [[0, 4], [14, 16], [51, 55]],\n",
       "  'NFP': [[5, 8], [17, 27], [36, 40], [45, 50], [56, 59], [60, 66]],\n",
       "  'VBZ': [[33, 35]]},\n",
       " 'syntax': {'NP': [[5, 27], [36, 50], [56, 66]],\n",
       "  'NP-SBJ': [[28, 32]],\n",
       "  'PP': [[0, 27], [51, 66]],\n",
       "  'PP-LOC': [[14, 27]],\n",
       "  'VP': [[33, 66]],\n",
       "  'X-PRD': [[36, 66]]},\n",
       " 'text': 'From NBC news in Washington this is Meet The Press with Tim Russet.'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85a17d56-509c-437b-abf6-015399ef1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_string = entry[\"text\"]\n",
    "this_doc = nlp(\"\".join(this_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "334fe3f3-e2c2-4829-9e5a-b3b84251783b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "From NBC news in Washington this is Meet The Press with Tim Russet."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c27fab75-0eb7-4539-9b46-b5153fcedf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NBC, Washington, Meet The Press, Tim Russet)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bded3595-8e1a-448b-8545-39ae7f9f6d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ent = this_doc.ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b8886c1a-f720-4573-b55d-f96d629254c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6f9ad0c-5ea7-4d83-9d0d-aab7db7858d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ace05caa-c930-4726-951c-3983cb8f8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = \"data/corpus_average.pkl\"\n",
    "with open(read_file, \"rb\") as fin:\n",
    "    corpus = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "57c443c5-ab41-4b2e-81aa-8a0c563cefd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1864, 768)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['TESTING'][0].shape # embedding of TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aa9b1bcf-d081-4f30-ad16-7a99cd207012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus['TESTING'][1]) # labels of TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d0716-40c2-4fcf-9a2c-4ad16ffed91f",
   "metadata": {},
   "source": [
    "### ContextualVectors(Pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "26bb4540-0ba4-4cc4-984b-bb2c2efbe6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "From NBC news in Washington this is Meet The Press with Tim Russet."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ee39f7fb-46cf-4ca2-9d58-e200545a596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = this_doc._.trf_data.align.lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "82cb28ca-c8cb-409e-98cc-ffe19b6064dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths # sub-words lengths of entities (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2766537f-33a2-4118-895f-53c58bccd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = this_doc._.trf_data.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "86ec8c66-2667-4507-bf5d-df6c4f39c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 17, 768)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[0].shape # vectors of sub-words (ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "786ad8a1-0ac3-4dfc-a720-2d387b62e345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5b458269-dc3a-4cbd-ba4b-54e1d3624192",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = this_doc._.trf_data.tokens['input_texts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5a000a8d-9e25-42c5-a4da-c8c30ec99b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'From',\n",
       " 'ĠNBC',\n",
       " 'Ġnews',\n",
       " 'Ġin',\n",
       " 'ĠWashington',\n",
       " 'Ġthis',\n",
       " 'Ġis',\n",
       " 'ĠMeet',\n",
       " 'ĠThe',\n",
       " 'ĠPress',\n",
       " 'Ġwith',\n",
       " 'ĠTim',\n",
       " 'ĠRuss',\n",
       " 'et',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts # sub-words tokens (doc) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a972f5c2-0694-4321-9c71-222b408e98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf_vector = tensors[0][0][13:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9beb517d-8bc2-4714-bd1e-8a174a02aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c54cd5c4-5538-4f07-b023-dd4661119b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(trf_vector, axis=0).shape # average vector of subwords vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b33b47-5ad8-4d8b-bd69-3b35a0fbb7ff",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "22fb46cd-281f-4869-a9b4-b2f26f108747",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels_ = corpus[\"TRAINING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f5e77fd5-237b-452a-a741-72b493541154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'cupy._core.core.ndarray'>\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(type(train_data)) # indicate GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c536b49-ee33-43bc-a5cf-be3bbced3aa8",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b1dfd5-cbac-4095-96da-7cd60ccb743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa83f62-fb39-4ee6-a5b8-c716998a85f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "11.3\n",
      "8200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b6132f2-f5af-4daf-9276-4fe51bdf6c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14652ad-d377-40ea-968a-9af087c45735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210fa39-655c-4bf5-b721-063f2f69d375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
